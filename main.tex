% Template for PLoS
% Version 3.6 Aug 2022
%
% % % % % % % % % % % % % % % % % % % % % %
%
% -- IMPORTANT NOTE
%
% This template contains comments intended 
% to minimize problems and delays during our production 
% process. Please follow the template instructions
% whenever possible.
%
% % % % % % % % % % % % % % % % % % % % % % % 
%
% Once your paper is accepted for publication, 
% PLEASE REMOVE ALL TRACKED CHANGES in this file 
% and leave only the final text of your manuscript. 
% PLOS recommends the use of latexdiff to track changes during review, as this will help to maintain a clean tex file.
% Visit https://www.ctan.org/pkg/latexdiff?lang=en for info or contact us at latex@plos.org.
%
%
% There are no restrictions on package use within the LaTeX files except that no packages listed in the template may be deleted.
%
% Please do not include colors or graphics in the text.
%
% The manuscript LaTeX source should be contained within a single file (do not use \input, \externaldocument, or similar commands).
%
% % % % % % % % % % % % % % % % % % % % % % %
%
% -- FIGURES AND TABLES
%
% Please include tables/figure captions directly after the paragraph where they are first cited in the text.
%
% DO NOT INCLUDE GRAPHICS IN YOUR MANUSCRIPT
% - Figures should be uploaded separately from your manuscript file. 
% - Figures generated using LaTeX should be extracted and removed from the PDF before submission. 
% - Figures containing multiple panels/subfigures must be combined into one image file before submission.
% For figure citations, please use "Fig" instead of "Figure".
% See http://journals.plos.org/plosone/s/figures for PLOS figure guidelines.
%
% Tables should be cell-based and may not contain:
% - spacing/line breaks within cells to alter layout or alignment
% - do not nest tabular environments (no tabular environments within tabular environments)
% - no graphics or colored text (cell background color/shading OK)
% See http://journals.plos.org/plosone/s/tables for table guidelines.
%
% For tables that exceed the width of the text column, use the adjustwidth environment as illustrated in the example table in text below.
%
% % % % % % % % % % % % % % % % % % % % % % % %
%
% -- EQUATIONS, MATH SYMBOLS, SUBSCRIPTS, AND SUPERSCRIPTS
%
% IMPORTANT
% Below are a few tips to help format your equations and other special characters according to our specifications. For more tips to help reduce the possibility of formatting errors during conversion, please see our LaTeX guidelines at http://journals.plos.org/plosone/s/latex
%
% For inline equations, please be sure to include all portions of an equation in the math environment.  For example, x$^2$ is incorrect; this should be formatted as $x^2$ (or $\mathrm{x}^2$ if the romanized font is desired).
%
% Do not include text that is not math in the math environment. For example, CO2 should be written as CO\textsubscript{2} instead of CO$_2$.
%
% Please add line breaks to long display equations when possible in order to fit size of the column. 
%
% For inline equations, please do not include punctuation (commas, etc) within the math environment unless this is part of the equation.
%
% When adding superscript or subscripts outside of brackets/braces, please group using {}.  For example, change "[U(D,E,\gamma)]^2" to "{[U(D,E,\gamma)]}^2". 
%
% Do not use \cal for caligraphic font.  Instead, use \mathcal{}
%
% % % % % % % % % % % % % % % % % % % % % % % % 
%
% Please contact latex@plos.org with any questions.
%
% % % % % % % % % % % % % % % % % % % % % % % %

\documentclass[10pt,letterpaper]{article}
\usepackage[top=0.85in,left=2.75in,footskip=0.75in]{geometry}

% amsmath and amssymb packages, useful for mathematical formulas and symbols
\usepackage{amsmath,amssymb}

% Use adjustwidth environment to exceed column width (see example table in text)
\usepackage{changepage}

% textcomp package and marvosym package for additional characters
\usepackage{textcomp,marvosym}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{cite}

% Use nameref to cite supporting information files (see Supporting Information section for more info)
\usepackage{nameref,hyperref}

% line numbers
\usepackage[right]{lineno}

% ligatures disabled
\usepackage[nopatch=eqnum]{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% color can be used to apply background shading to table cells only
\usepackage[table]{xcolor}

% array package and thick rules for tables
\usepackage{array}

% create "+" rule type for thick vertical lines
\newcolumntype{+}{!{\vrule width 2pt}}

% create \thickcline for thick horizontal lines of variable length
\newlength\savedwidth
\newcommand\thickcline[1]{%
  \noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
  \cline{#1}%
  \noalign{\vskip\arrayrulewidth}%
  \noalign{\global\arrayrulewidth\savedwidth}%
}

% \thickhline command for thick horizontal lines that span the table
\newcommand\thickhline{\noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
\hline
\noalign{\global\arrayrulewidth\savedwidth}}


% Remove comment for double spacing
%\usepackage{setspace} 
%\doublespacing

% Text layout
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 5.25in 
\textheight 8.75in

% Bold the 'Figure #' in the caption and separate it from the title/caption with a period
% Captions will be left justified
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,justification=raggedright,singlelinecheck=off]{caption}
\renewcommand{\figurename}{Fig}

% Use the PLoS provided BiBTeX style
\bibliographystyle{plos2015}

% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother



% Header and Footer with logo
\usepackage{lastpage,fancyhdr,graphicx}
\usepackage{epstopdf}
%\pagestyle{myheadings}
\pagestyle{fancy}
\fancyhf{}
%\setlength{\headheight}{27.023pt}
%\lhead{\includegraphics[width=2.0in]{PLOS-submission.eps}}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
\fancyheadoffset[L]{2.25in}
\fancyfootoffset[L]{2.25in}
\lfoot{\today}

%% Include all macros below

\newcommand{\lorem}{{\bf LOREM}}
\newcommand{\ipsum}{{\bf IPSUM}}

%% END MACROS SECTION


\begin{document}
\vspace*{0.2in}

% Title must be 250 characters or less.
\begin{flushleft}
{\Large
\textbf\newline{TCRBinder: unified pre-trained language model with paired-chain coherence for predicting T-cell receptor binding specificity} % Please use "sentence case" for title and headings (capitalize only the first word in a title (or heading), the first word in a subtitle (or subheading), and any proper nouns).
}
\newline
% Insert author names, affiliations and corresponding author email (do not include titles, positions, or degrees).
\\
Weihe Dong\textsuperscript{1},
Qiang Yang\textsuperscript{1,2},
Long Xu\textsuperscript{1},
Xiaokun Li\textsuperscript{1,3,4,5*},
Kuanquan Wang\textsuperscript{1},
Suyu Dong\textsuperscript{6*},
Gongning Luo\textsuperscript{1},
Xianyu Zhang\textsuperscript{7},
Tiansong Yang\textsuperscript{8},
Xin Gao\textsuperscript{9},
Guohua Wang\textsuperscript{1*}

\bigskip
\textbf{1} School of Computer Science and Technology, Harbin Institute of Technology, Harbin, 150001, China
\\
\textbf{2} Zhengzhou Research Institute, Harbin Institute of Technology, Zhengzhou, 450000, China
\\
\textbf{3} School of Computer Science and Technology, Heilongjiang University, Xuefu Road, Harbin, 150080, China
\\
\textbf{4} Postdoctoral Program of Heilongjiang Hengxun Technology Co., Ltd., Xuefu Road, 150090, Harbin, China
\\
\textbf{5} Shandong Hengxun Technology Co., Ltd., Miaoling Road, 266100, Qingdao, China
\\
\textbf{6} College of Computer and Control Engineering, Northeast Forestry University, Hexing Road, 150004, Harbin, China
\\
\textbf{7} Department of Breast Surgery, Harbin Medical University Cancer Hospital, Harbin 150081, China
\\
\textbf{8} Department of Rehabilitation, The First Affiliated Hospital of Heilongjiang University of Traditional Chinese Medicine, Xuefu Road, 150040, Harbin, China
\\
\textbf{9} Computer Science Program, Computer, Electrical and Mathematical Sciences and Engineering Division, King Abdullah University of Science and Technology (KAUST), Thuwal 23955-6900, Kingdom of Saudi Arabia
\\
\bigskip

% Use the asterisk to denote corresponding authorship and provide email address in note below.
* 24B303024@stu.hit.edu.cn; dongsuyu@126.com; ghwang@hit.edu.cn

\end{flushleft}
% Please keep the abstract below 300 words
\section*{Abstract}
 Deciphering how human T cells recognise peptide–HLA (pHLA) complexes underpins next-generation vaccines and personalised immunotherapies, yet extreme sequence diversity and paired chains interdependence still hamper reliable in silico prediction of T-cell receptor (TCR) specificity. To overcome these hurdles we built TCRBinder, a paired chain aware deep model with a multi-branch encoder that routes each molecular component through dedicated transformer-based modules to capture contextual signals in both HLA pseudo-sequences and antigenic peptides and simultaneously processes the TCR $\alpha$ and $\beta$ chains. By doing so, chain coherence is enforced to emulate peptide–HLA–TCR (PHT) interactions and expose residue-level contact motifs. Across PHT and peptide–TCR (pTCR) benchmarks, the model delivered state-of-the-art performance (AUC-ROC = 0.944, AUPR = 0.855 for the PHT task) and remained superior on multiple independent datasets. Our results closely monitored the dynamics of clonal expansion and, in a large SARS-CoV-2 repertoire containing wholly unseen peptides, improved the AUC-ROC by up to 17.4\% over the leading alternatives. Moreover, TCRBinder provided mechanistic insight by pinpointing contact hot-spots and quantifying residue contributions to binding probability. These capabilities position TCRBinder as a versatile tool for rational antigen discovery, immunotherapy stratification, and neoantigen vaccine design.\\


% Please keep the Author Summary between 150 and 200 words
% Use first person. PLOS ONE authors please skip this step. 
% Author Summary not valid for PLOS ONE submissions.   
\section*{Author summary}
T-cell receptors (TCRs) play a central role in adaptive immunity by recognizing peptide–HLA complexes, but predicting their binding specificity has remained a formidable challenge due to sequence diversity and the cooperative action of $\alpha$ and $\beta$ chains. Existing computational methods often rely on limited CDR3$\beta$ information or single-chain inputs, which restrict predictive accuracy and generalization. To address this, we developed TCRBinder, a unified deep learning framework that processes full-length paired TCR chains together with peptide and HLA sequences. The model combines pre-trained encoders, ESM2 and RoFormer, with a multi-fusion convolutional network to capture both global semantic context and local interaction motifs. Across benchmark and independent datasets, TCRBinder consistently surpasses leading tools, demonstrating state-of-the-art performance and robust transferability to unseen epitopes and external clinical repertoires. Importantly, its predictions correlate with in vivo clonal expansion and highlight key residues that shape recognition, providing interpretable insights into molecular binding mechanisms. By integrating predictive accuracy with biological relevance, TCRBinder offers a versatile platform for antigen discovery, immunotherapy design, and the development of personalized vaccines.

\linenumbers

% Use "Eq" instead of "Equation" for equation citations.
\section*{Introduction}
T-cell receptor (TCR) based immunotherapies, including checkpoint inhibitors and engineered T cell therapies, have revolutionized cancer treatment by leveraging neoantigens to trigger potent immune responses \cite{ref1,ref2,ref3}. The effectiveness of these therapies relies on the precise recognition of antigenic peptides presented by human leukocyte antigen (HLA) molecules, which initiate T cell activation and tumor cell elimination \cite{ref4,ref5,ref6}. However, the limited response rates in many patients stem from challenges in identifying neoantigens that elicit strong TCR binding \cite{ref7,ref8,ref9}. In the cancer immunity cycle, TCR recognition with antigenic peptide–HLA (pHLA) complexes is a critical step for T cell activation and effector function \cite{ref10,ref11}. Experimental techniques, including tetramer staining and high-throughput sorting, remain labor intensive and time costly, underscoring the need for scalable computational frameworks to predict TCR binding specificity and facilitate individual immunotherapy \cite{ref12,ref13,ref14}.


The binding of TCRs to pHLA complexes is orchestrated by the synergistic interplay of the TCR $\alpha$ and $\beta$ chains, with complementarity-determining region 3 (CDR3) serving as the primary determinant of specificity \cite{ref15,ref16}. The human TCR repertoire exhibits immense diversity, with an estimated $10^{15}$ to $10^{61}$ possible configurations arising from random recombination \cite{ref17,ref18}. This vast sequence variability, coupled with the extensive polymorphism of HLA molecules, poses significant hurdles for computational modeling in antigen presentation and TCR recognition \cite{ref19,ref20}. To achieve precise pHLA-TCR (PHT) binding prediction, models must capture the cooperative dynamics of TCR $\alpha$ and $\beta$ chains and their nuanced interactions with diverse pHLA complexes, necessitating advanced deep learning solutions.


Current computational approaches for pHLA and peptide–TCR (pTCR) binding prediction have made notable progress but face critical limitations \cite{ref21}. Tools like HLAIImaster\cite{ref22}, MHLAPre\cite{ref23}, NetMHCpan4.0\cite{ref24} and MHCflurry\cite{ref25} excel in predicting pHLA binding using deep neural networks, while pTCR prediction models, such as ImRex\cite{ref26}, TEIM\cite{ref27}, and TCR-AI\cite{ref28}, focus on peptide–CDR3 interactions, often neglecting HLA context or TCR chain synergy. Emerging methods like pMTnet\cite{ref29} and THLANet\cite{ref30} aim to model PHT relationships but are constrained by single chain TCR representations or specific HLA alleles, limiting their ability to address paired chain dynamics. The scarcity of validated binding data further exacerbates these challenges, highlighting the need for a unified model that integrates comprehensive interaction data to improve predictive performance.


To overcome above-mentioned problems, we introduce TCRBinder, a paired chain aware deep learning framework that harnesses pre-trained protein language models to predict PHT binding specificity with improved accuracy. TCRBinder employs a multi-branch encoder architecture to independently process TCR $\alpha$ and $\beta$ chains, HLA pseudo-sequences, and antigen peptides, utilizing ESM2 and RoFormer modules to learn rich, modality-specific representations. These high-dimensional representations are then integrated via a multi-branch fusion block with Multi-Fusion Convolutional Neural Networks (MFCNNs). This unified approach enables the simultaneous prediction of PHT and pTCR binding specificity, providing interpretable insights into residue-level interactions and offering a versatile platform for neoantigen discovery and immunotherapy design.


TCRBinder outperforms existing methods in prediction of the PHT and pTCR binding specificities, delivering three transformative contributions. First, its unified architecture facilitates integrated prediction of PHT and pTCR interactions, enabling a comprehensive evaluation of neoantigen immunogenicity that enhances understanding of T cell activation. This flexibility supports multiple tasks, including pTCR and PHT binding, within a single framework. Second, we employ pre-training on large-scale protein datasets and multitask fine-tuning on curated TCR repositories, such as VDJdb \cite{ref31} and IEDB \cite{ref32}, to overcome data scarcity and ensure robustness across diverse HLA alleles. Third, extensive validation on independent datasets demonstrates more than 17.4\% improvement in Area Under the Curve–Receiver Operating Characteristic (AUC-ROC) over state-of-the-art models, with saliency maps elucidating key residue interactions. These advancements position TCRBinder to streamline neoantigen screening, advance TCR-engineered therapies, and provide molecular insights into immune recognition, laying the groundwork for precision immunotherapy.


\section*{Results}
\subsection*{TCRBinder Overview}
% For figure citations, please use "Fig" instead of "Figure".
T-cell receptors (TCRs) play a pivotal role in adaptive immunity by recognizing peptide antigens presented on human leukocyte antigen (HLA) molecules, enabling T cells to distinguish self from non-self and target aberrant cells, such as those in cancer (Fig. \ref{fig1}a). This recognition process involves intricate interactions between the TCR $\alpha$ and $\beta$ chains, the antigenic peptide, and the HLA molecule, culminating in T-cell activation and effector functions like cytotoxicity against tumor cells \cite{ref33,ref34,ref35}. To computationally predict TCR binding specificity with high fidelity, we developed TCRBinder, a unified pre-trained language model that incorporates paired chain coherence, departing from conventional methods that focus narrowly on CDR3$\beta$ regions. By modeling full-length TCR $\alpha$ and $\beta$ chains alongside peptide and HLA sequences, TCRBinder captures the biophysical nuances of ternary complex formation, including long-range dependencies and cross-chain interactions.


The core of TCRBinder is four specialized encoders that pre-trained on vast immunological datasets to learn modality-specific embeddings (Fig. \ref{fig1}b). The $\alpha$-RoFormer and $\beta$-RoFormer, transformer-based encoders with rotary positional embedding, are pre-trained on millions of unpaired full-length TCR sequences using masked language modeling to reconstruct corrupted inputs, thereby learning chain-specific contextual representations while enforcing coherence for paired chains during downstream tasks (Fig. \ref{fig1}d). In parallel, the Antigen-ESM2 and HLA-ESM2 encoders, derived from the large-scale protein language model ESM2 \cite{ref36}, undergo fine-tuning on extensive corpora of antigenic peptides and HLA pseudo-sequences, employing stacked self-attention layers to extract residue-level dependencies and polymorphic patterns (Fig. \ref{fig1}c). This pre-training strategy ensures that each encoder specializes in its respective domain, with the peptide encoder capturing positional motifs, the HLA encoder modeling allele-specific binding grooves, and the TCR encoder focusing on variable and complementary determining regions of the TCR chains, which together yield robust and transferable features for binding prediction.

% Place figure captions after the first paragraph in which they are cited.
\begin{figure}[!h]
    \includegraphics[scale=0.54]{1.eps}
    \caption{Overview of TCRBinder and its components. (\textbf{a}) Schematic of T cell-mediated cytotoxicity. A T cell utilizes its T cell receptor (TCR) to recognize an antigen presented by an HLA molecule on a cancer cell. This specific interaction, illustrated in the magnified view of the PHT complex, is the critical event that triggers the destruction of the cancer cell. (\textbf{b}) The framework of the PHT binding specificity predictor. It takes the antigen, HLA, and TCR $\alpha$ and $\beta$ chain sequences as input. Each sequence is encoded by passing through a pre-trained encoder (ESM2 for antigen and HLA; Roformer for TCR chains) and a Multi-Fusion Convolutional Neural Network (MFCNN) feature extractor. A multilayer perceptron model (MLP) finally fuses the features from all four sequences to predict the binding specificity. (\textbf{c}) Internal architecture of the antigen and HLA encoders. The ESM2-based models for both antigen and HLA consist of multiple stacked self-attention layers, which are pre-trained on large protein sequence databases to learn generalizable representations. (\textbf{d}) Schematic of the self-supervised pre-training of the TCR Roformer. Unlabeled TCR chain sequences are used to pre-train the TCR Roformer via masked language modeling. The model is trained to predict the identity of masked tokens, thereby learning generalizable representations of TCR sequences. (\textbf{e}) The architecture of the MFCNN. The MFCNN module integrates convolutional and fully connected layers to extract high-level biological features from the encoder outputs. Created with \href{https://app.biorender.com}{BioRender}.}
\label{fig1}
\end{figure}

These specialized embedding representations are integrated via a multi-branch fusion block, which utilize multi-scale CNN layers to process spatially aligned feature, followed by reshaping and fully connected layers to learn the high-level biological features. Subsequently, these captured features are concatenated and passed through a multilayer perceptron (MLP) network to predict PHT binding specificity (Fig. \ref{fig1}b and e). Supervised training on a curated dataset that comprises 20,938 experimentally validated PHT binders together with a tenfold larger set of approximately 210,000 \textit{in silico} negative decoys (\textbf{Methods}), which collectively optimize the model for accurate discrimination. This architecture facilitates cross-modal learning of interaction determinants and enhances generalization to unseen epitopes and donor repertoisres, as it outperforms CDR3$\beta$-centric baselines by leveraging holistic chain-level information and reducing overfitting to sparse binding data.


\subsection*{Evaluation of Binding Prediction Performance}
PHT binding specificity plays a pivotal role in expediting immunotherapies that pinpoint robust T-cell responses to emerging threats or cancers. To rigorously benchmark TCRBinder's proficiency in forecasting PHT binding specificity,  we created a robust repository of full-sequence PHT engagements. This corpus encompasses vast TCR recognized antigens ranging from 8 to 12 residues (Fig. \ref{fig2}a) and diverse HLA alleles. As observed, a handful of privileged pHLA complexes monopolize the binding landscape, which is exemplified by HLA-A11:01:AVFDRKSDAK and HLA-A02:01:GILGFVFTL that engage myriad TCR variants and epitomize immunodominant biases (Fig. \ref{fig2}b and c). This asymmetrical, trailing contour reflects the complex patterns of PHT binding specificity in nature, ensuring the validity of model evaluation. Given that TCR interrogation hinges on prior peptide stabilization within HLA pockets, we scrutinized TCRBinder's aptitude for peptide–HLA affinity estimation against leading comparators, including STAPLER \cite{ref37}, NetTCR v2.2 \cite{ref38}, MixTCRpred \cite{ref39}, and NetTCR v2.0 \cite{ref40}. This evaluation utilizes a dataset of 20,938 PHT complexes spanning 256 distinct peptides and 27 HLA subtypes (\textbf{Methods}).


We employ golden-standard evaluation metrics, including the Area Under the Curve – Receiver Operating Characteristic (AUC-ROC) and the Area Under the Precision-Recall Curve (AUPR), F1-Score, and accuracy to comprehensively assess the model performance (As shown in Section A in \nameref{S1_Text}). To ensure fair and consistent evaluation, we compared our model with state-of-the-art baseline methods (As shown in Section B in \nameref{S1_Text}) under identical datasets and training strategies. TCRBinder excelled with an AUC-ROC of 0.944 and AUPR of 0.855 on the evaluation partition (Fig. \ref{fig2}d), outperforming STAPLER (AUC-ROC: 0.841, AUPR: 0.762), NetTCR v2.2 (AUC-ROC: 0.897, AUPR: 0.783), MixTCRpred (AUC-ROC: 0.863, AUPR: 0.640), and NetTCR v2.0 (AUC-ROC: 0.809, AUPR: 0.719) by margins of 5.2–16.7\% in AUC-ROC and 9.2–18.9\% in AUPR. Additionally, TCRBinder maintained strong accuracy, specificity, and F1-score (Supplementary Fig. S1 and Table S1 in \nameref{S1_Text}), attesting to the stability and reliability of its predictive framework. Broadening the lens to pTCR dynamics, the framework sustained dominance, yielding an AUC-ROC of 0.924 and AUPR of 0.825 (Supplementary Fig. S2 in \nameref{S1_Text}), underscoring its versatility in unraveling intricate recognition hierarchies. 

To determine whether the pre-training stage furnishes the representational structure needed to meet this challenge, we first inspected the latent space learned by TCRBinder. Pre-training markedly improves representation quality. Using UMAP to visualize embeddings, the untrained model (Fig. \ref{fig3}a) yields scattered, poorly separable features, whereas the pre-trained model (Fig. \ref{fig3}b) forms compact, epitope specific clusters. This clearer structure indicates larger inter epitope differences are being captured and that pre-training strengthens the model’s ability to extract discriminative features for epitope recognition. Diverse antigen sizes and specificities of HLA alleles usually dictate distinct binding affinities, which are crucial elements affecting the accuracy of peptide–HLA interaction forecasting. 

% Place figure captions after the first paragraph in which they are cited.
\begin{figure}[!h]
    \includegraphics[scale=0.44]{2.eps}
    \caption{Dataset Characteristics and Performance Benchmarking. (\textbf{a}) Frequency distribution of epitope peptide lengths in the benchmark dataset. (\textbf{b}) Distribution of TCRs recognizing epitopes restricted to different HLA alleles in the benchmark dataset. (\textbf{c}) Number of binding $\alpha$$\beta$TCRs for the top 20 pHLAs with the most experimentally validated TCRs. (\textbf{d}) AUC-ROC and AUPR of TCRBinder and state-of-the-art methods on the benchmark test dataset. (\textbf{e}) AUC-ROC and AUPR of TCRBinder and state-of-the-art methods on the VDJdb test dataset. (\textbf{f}) AUC-ROC and AUPR of TCRBinder and state-of-the-art methods on the SARS-CoV-2 dataset.}
\label{fig2}
\end{figure}

\subsection*{Generalization Experiments}

We conducted two evaluation scenarios to assess the generalization ability of TCRBinder in predicting PHT binding specificity, designed to mimic real world applications. The first scenario focused on predicting binding TCRs for previously unseen epitopes, while the second scenario evaluated the ability to predict the binding specificity of a distinct TCR population from the VDJdb database \cite{ref31}. Unlike methods that rely solely on single-chain CDR3 regions, TCRBinder was trained on full-length paired TCR $\alpha$ and $\beta$ sequences, allowing it to capture a more comprehensive set of structural and sequence determinants that govern pHLA recognition. 

% This design enables the model to exploit inter-chain co-adaptation signals and sequence context beyond the hypervariable loops, which are critical for robust prediction in challenging generalization tasks.

In the unseen-epitope setting, TCRBinder achieved an AUC-ROC of 0.627 and an AUPR of 0.329 (Supplementary Fig. S3 in \nameref{S1_Text}), substantially outperforming the comparative baseline methods STAPLER \cite{ref37}, NetTCR v2.2 \cite{ref38}, MixTCRpred \cite{ref39},  and NetTCR v2.0 \cite{ref40}. Beyond these area-based metrics, classification performance also demonstrated the model’s strength, with consistently high accuracy, specificity, and F1-score (Supplementary Fig. S4 and Table S2 in \nameref{S1_Text}). These results indicated that TCRBinder can effectively generalize to novel antigenic targets outside its training distribution, a capability essential for applications such as emerging pathogen surveillance and cancer neoantigen discovery. In the second setting, using unique PHT pairs from the VDJdb database, TCRBinder maintained strong predictive performance with an AUC-ROC of 0.6618 and an AUPR of 0.4250 (Fig. \ref{fig2}e). The model again exhibited high accuracy, specificity, and F1-score across this independent evaluation (Supplementary Fig. S5 and Table S3 in \nameref{S1_Text}), demonstrating its robustness across different data sources and TCR repertoires. The ability to sustain performance in the VDJdb evaluation suggests that TCRBinder captures generalizable sequence–structure–specificity relationships that are transferable across populations, experimental platforms, and epitope repertoires.

These generalization experiments highlight TCRBinder’s ability to handle both epitope level novelty and repertoire level diversity. Its use of full-length paired TCR $\alpha$ and $\beta$ chains enables richer feature representation than single-chain approaches, enhancing recognition of binding determinants that remain consistent across unseen peptides and distinct TCR populations. This robustness, coupled with superior classification metrics compared to other advanced methods, underscores its suitability for real-world deployment in immunotherapy development, epitope prioritization, and immune repertoire monitoring.  


\subsection*{SARS-CoV-2-responsive PHT Dataset}
To further investigate the application potential of TCRBinder, we evaluated its performance on a clinically relevant, external PHT recognition dataset. This dataset was constructed using samples of SARS-CoV-2-responsive T cells \cite{ref41}, allowing for a robust assessment of the model's ability to identify antigen-specific TCRs from a complex repertoire. We benchmarked TCRBinder against several other state-of-the-art pan-specific models, including STAPLER \cite{ref37}, NetTCR v2.2 \cite{ref38}, MixTCRpred \cite{ref39},  and NetTCR v2.0 \cite{ref40}, to assess its predictive power in a direct comparison. TCRBinder delivered a remarkable and consistently superior performance across all metrics. Specifically, it achieved the AUC-ROC of approximately 0.648 and also secured the AUPR of around 0.410 (Fig. \ref{fig2}f), TCRBinder again exhibited high accuracy, specificity, and F1-score across this independent evaluation (Supplementary Fig. S6 and Table S4 in \nameref{S1_Text}). 

TCRBinder's superior ability to identify TCRs responsive to viral epitopes is attributable to its core design choices, including its use of full-length paired TCR $\alpha$ and $\beta$ chain sequences, pre-training-derived priors, and position-aware encoding. Together, these features allow the model to capture complex inter-chain complementarity and contextual information that extends far beyond the TCR $\beta$ chain CDR3 region alone. In practical terms, these results indicate that TCRBinder can effectively generalize from training distributions to real-world clinical repertoires exposed to diverse SARS-CoV-2 antigens, maintaining robust discrimination across individuals and targets. This thereby supports its powerful application in immune monitoring and enabling a rapid response to emerging viral variants.


\subsection*{Ablation Experiments}

To quantify the contribution of different architectural components to TCRBinder, we constructed three ablated variants: (1) without the TCR $\alpha$ chain (CDR3-only), in which only the TCR $\beta$ chain CDR3 sequence was provided as input; (2) without the TCR $\alpha$ chain (beta-only), in which the TCR $\beta$ chain was utilized but the TCR $\alpha$ chain was removed; and (3) without ESM2 or RoPE, where pretrained protein embeddings or rotary positional encoding were discarded and replaced with one-hot tokenization or absolute-positional embeddings. Compared with variants that feed only the TCR $\beta$ chain CDR3 (AUC-ROC = 0.808, AUPR = 0.701) or the full TCR $\beta$ chain (AUC-ROC = 0.871, AUPR = 0.759) into the model, using the full-length paired TCR $\alpha$ and $\beta$ chains as input consistently delivered the highest discriminative power (approximately AUC-ROC = 0.944, AUPR = 0.855), demonstrating great gains from paired chain coherence and extended sequence context(Supplementary Table S5 in \nameref{S1_Text}). The TCR $\alpha$ chain CDR1 and CDR2 loops frequently contact the HLA $\alpha$-helices and the peptide backbone, contributing stabilizing interactions and fine-grained specificity that TCR $\beta$ CDR3 alone cannot fully capture; therefore, TCR $\alpha$ and $\beta$ chains pairing supplies decisive information for pHLA recognition. At the representation learning level, simultaneously removing ESM2 and RoPE and substituting them with one-hot encodings and absolute positions led to further degradation, indicating that pretrained semantic priors and relative positional encoding jointly facilitate modeling of long-range dependencies across loops and across chains, as well as their alignment with peptide–HLA features.

% Place figure captions after the first paragraph in which they are cited.
\begin{figure}[!h]
    \includegraphics[scale=0.6]{3.eps}
    \caption{TCRBinder Captures Antigen-Specific Features and Correlates with Clonal Expansion. (\textbf{a-b}) UMAP projections of TCR sequence embeddings, obtained from the and the Initialized RoFormer (a) and the Pre-trained RoFormer (b). (\textbf{c}) Clone-frequency distributions for pHLAs across four donors profiled with 10x Genomics single-cell Peptide–HLA capture. (\textbf{d}) In the 10x Genomics Chromium immune-profiling dataset, the proportion of T-cell clones shows a significant positive correlation with the TCRBinder-predicted binding score for the corresponding pHLA across all four donors.}
\label{fig3}
\end{figure}

\subsection*{Prediction of TCRBinder highly Correlates with T-Cell Clonal Expansion}

To further validate our model, we assessed whether the PHT binding specificity predicted by TCRBinder reflect antigen-driven T-cell proliferation \textit{in vivo}. We utilized a single-cell immune profiling dataset generated using the 10x Genomics Chromium platform \cite{ref42}, which profiled T cells from four healthy donors against a panel of 44 distinct pHLAs.

For each T cell, its antigen specificity was determined by counting the Unique Molecular Identifiers (UMIs) associated with each pHLA. A PHT binding event was classified as a positive interaction if its UMI count was 10 or greater, and cells with UMI counts below this threshold for a given pHLA were not considered in the analysis for that specific interaction (Fig. \ref{fig3}c). TCRBinder then generated binding scores for each T-cell clonotype against all 44 pHLAs. To avoid compositional effects across donors, we used Spearman rank correlation to evaluate the association between the highest predicted binding score for each clone and its corresponding clonal expansion.


As shown in the (Fig. \ref{fig3}d), we observed a consistent and statistically significant positive correlation between T-cell clone proportion and the predicted binding scores across all four donors. This finding strongly aligns with the biological expectation that TCRs with higher binding affinity are more prone to undergo clonal selection and expansion. The consistent trend across multiple donors confirms that our model's predictions are biologically relevant and validates its ability to identify TCRs that are likely to mount a functional response. In summary, TCRBinder's binding predictions capture meaningful determinants of antigen-specific clonal dynamics within human T-cell repertoires, underscoring the model's strong relevance for adaptive immune response.

% Place figure captions after the first paragraph in which they are cited.
\begin{figure}[!h]
    \includegraphics[scale=0.6]{4.eps}
    \caption{Structural and Energetic Analysis of pHLA-TCR Recognition. (\textbf{a}) Analysis of TCRBinder score perturbation across five segments of the CDR3$\beta$ loop. (\textbf{b}) Calculated change in binding free energy ($\Delta$$\Delta$G) upon \textit{in silico} alanine scanning of key residues in the 1OGA complex. (\textbf{c}) Structure of the 1OGA PHT complex (i) with a magnified view of the interaction interface (ii) and structural details for each mutation site, i.e., R98A, S99A, S100, and E102A (iii). (\textbf{d}) Structural visualizations of three additional PHT complexes (PDB IDs: 5HHO, 5NHT, 3KPS).}
\label{fig4}
\end{figure}

\subsection*{Key Binding Site Analysis}


To systematically investigate the predictive mechanisms of the TCRBinder model and precisely pinpoint key amino acid residues that regulate TCR recognition, we integrated computational alanine scanning with structure-based analysis. We began by dividing the CDR3 region of the TCR $\beta$ chain into five segments and analyzed the changes in model scores resulting from site directed alanine mutations within each segment. As illustrated in (Fig. \ref{fig4}a), the most significant score changes were concentrated in the central portions of the CDR3$\beta$ chain (segments 3 and 4), while the terminal segments exhibited minimal impact. This finding is consistent with classical structural immunology principles, which indicate that the central region of the peptide typically protrudes towards the TCR, and that the apex region of the CDR3$\beta$ chain interacts with this protruding segment. Consequently, residues located within this core region exert a decisive influence on recognition specificity and affinity.

To further validate this pattern and quantify the contribution of critical residues, we employed the HLA-A02:01-GILGFVFTL-TCR complex (PDB ID: 1OGA) as a specific case study, performing detailed in silico alanine scanning using Rosetta software. We sequentially substituted candidate residues on both the CDR3$\beta$ and the peptide with alanine and calculated the resulting changes in binding free energy ($\Delta$$\Delta$G). The results presented in (Fig. \ref{fig4}b) clearly delineate an energy importance hierarchy, which shows that the model-predicted key residues R98, S99, and S100, when substituted with alanine (R98A, S99A, S100A), all induced significant destabilizing effects (i.e., large $\Delta$$\Delta$G values). In contrast, the mutation at the non-critical site, E102A, resulted in only a negligible loss of energy. These energetic trends not only confirm the attribution accuracy of the TCRBinder model but also unequivocally designate the R98, S99, S100 combination as a "hotspot triplet" situated at the apex of CDR3$\beta$ loop.

Beyond binding energy analysis, we conducted structural analysis to further elucidate the physical basis for these energetic differences. The complete 1OGA ternary complex and its interface contact network are presented in (Fig. \ref{fig4}c). Residues R98, S99, and S100 at the CDR3$\beta$ apex are positioned in close proximity to the peptide's center and the HLA $\alpha$-helical groove, forming a dense network of close-range contacts. Following the substitution of these key residues with alanine, the shortest distances between the peptide and TCR generally increased, leading to a reduction in interface contact area. Taking the S99A mutation as an example, the shortest hydrogen bond distance between TCR-S99 and peptide-F5 increased significantly from 2.5 Å to 4.0 Å, whereas the geometric changes induced by the non-core site TCR-E102 mutation were considerably smaller. This geometric alteration is consistent with the $\Delta$$\Delta$G ranking in (Fig. \ref{fig4}b), supporting the critical role of these residues in interface stability. Finally, to test the generalizability of this finding, we applied this analytical framework to three additional complexes with distinct peptides and HLA alleles (PDB IDs: 5HHO, 5NHT, 3KPS). As shown in (Fig. \ref{fig4}d), we observed similar patterns dominated by recognition from the central region of the CDR3$\beta$, demonstrating the reliability and universality of this analytical approach in identifying core PHT interaction patterns.


\section*{Materials and methods}

\subsection*{Datasets}
The limitation of positive PHT binding data often motivates data aggregation from multiple sources to build a more substantial dataset for training and evaluation. We collected positive binding triples for training from four publicly available datasets: OTS \cite{ref43}, McPAS-TCR \cite{ref44}, VDJdb \cite{ref31}, IEDB \cite{ref32}. The positive dataset consists of 20,938 experimentally validated entries, each comprising a paired TCR $\alpha$ and $\beta$ chains CDR3 sequence, an HLA allele, and an antigenic peptide. These data were split into training, validation and test datasets according to the ratio of 8:1:1, respectively. All entries with duplicate records or anomalous sequences (including missing residues or ambiguous symbols) were removed to ensure data quality. We acquired two independent external data for testing with strict quality control and standardized preprocessing from the VDJdb database: (1) the binding TCRs for unseen epitopes and (2) the binding specificity of a distinct TCR population. We extracted the PHT binding pairs from publicly available datasets, which were obtained from T cells of SARS-CoV-2-infected patients and unexposed individuals stained with SARS-CoV-2-derived pHLA multimers. The statistical information of used datasets is provided in Supplementary Table S6 in \nameref{S1_Text}.

To rigorously evaluate the generalization capacity of different models on new sequences, we excluded all the PHT triples whose pHLA pairs were previously encountered in the training dataset. Additionally, we explored several negative sampling schemes recommended in systematic studies, including the following: (1) Randomly shuffled sequence triples from the positive data as negative samples, and (2) Unified epitope negative sampling in which the epitopes are sampled by their frequency distributions in the positive dataset. The negative samples generated by these two methods each account for approximately 50\% of the total negative samples. In our experiments, we generated negative samples that are ten times larger than the positive ones, creating a final training ratio of approximately 1:10 (positive:negative). This approach ensured a class balance conducive to robust model training and avoided potential biases from trivial mismatching. 

\subsection*{Model architecture}
In TCRBinder, we adopt a unified architecture that couples global context from protein language models with explicit local interaction modeling to predict PHT or PT binding. The antigenic peptide and HLA pseudo-sequence are encoded independently by ESM2, providing evolution-aware token embeddings that capture biochemical and positional regularities. The paired TCR $\alpha$ and $\beta$ chains are encoded by two RoFormer encoders that are pre-trained with masked language modeling on unlabeled TCR repertoires, using rotary positional embeddings to preserve order and long-range dependencies while learning chain-specific grammar. The four sequence embeddings are then aligned and concatenated into a joint feature by a Multi-branch Fusion Convolutional Neural Network (MFCNN) that serves as the interaction engine, which applies multi-scale 1D convolutions that sweep across and between channels to detect co-occurring motifs spanning peptide, HLA helices, and the CDR loops. In addition, residual connections, layer normalization, and dropout are used to stabilize optimization and enhance generalization, while a reshape layer combined with pointwise fully connected mixing is employed to integrate cross channel signals into compact feature maps. The MFCNN therefore aggregates short- and medium-range contact patterns while remaining computationally lightweight and robust to sequence length variation. Its outputs are summarized by global max pooling and fed to a MLP layer that produces the final binding specificity. This end-to-end design leverages pre-trained encoders for global semantics and the MFCNN for localized cross-sequence motifs, which together yield accurate predictions across diverse antigens and HLA alleles and support interpretation through activation based attributions that highlight salient residues at the PHT interface. The detailed hyper-parameters of TCRBinder are presented in Supplementary Table S7 in \nameref{S1_Text}.

\subsection*{Pre-training and Task-specific Adaptation of TCRBinder}
In both the large-scale pre-training and the downstream finetuning phases, we employ a unique amino acid (UAA) tokenizer tailored to the single residue sensitivity of T-cell receptor (TCR) sequences. Under the UAA scheme, every amino acid residue present in the corpus was assigned an exclusive token index, thereby establishing a strict one-to-one mapping between sequence characters and model inputs. Each token was encoded as an integer and subsequently transformed into a continuous vector representation through a trainable embedding matrix. This encoding approach allows the preservation of single-residue substitutions and small indels while maintaining sensitivity to positional context that is essential for capturing the subtle sequence variations that frequently modulate TCR recognition. Pre-training was conducted under the masked amino acid (MAA) paradigm, analogous to BERT’s masked language modeling objective. Specifically, 15\% of residues were randomly substituted with a special [mask] token, and the model was tasked with recovering the masked amino acids using only surrounding context, thereby enabling the network to learn local biochemical cues together with long-range structural dependencies in a fully self-supervised fashion. To encode paired chain TCR sequences, we adopted a Transformer-based backbone consisting of 12 stacked Roformer blocks, each equipped with 12 attention heads, a hidden size of 640, and a feed forward dimension of 2560, with GLU activation applied to hidden layers. Training was performed in 32-bit floating point precision (FP32) to ensure numerical stability. Both TCR $\alpha$ and $\beta$ chains models were trained on four NVIDIA GeForce A100 GPUs with data parallelism. In the pre-training, over 5.3 million paired chain TCR sequences with pairing information were used to optimize the $\alpha$-RoFormer and $\beta$-RoFormer encoders. We employed the AdamW optimizer with a learning rate of 5e-5, a warmup ratio of 0.1, resulting in a training duration of approximately 20 days. The detailed hyper-parameters of pre-training process are presented in Supplementary Table S8 in \nameref{S1_Text}.


\subsection*{ESM2-based Protein Representation}
To obtain high-quality protein sequence representations for antigenic peptides and HLA pseudo-sequences, we employed the large protein language model ESM2-150. It takes raw amino acid sequences as input and generates residue-level embeddings through a transformer-based architecture pretrained with a masked language modeling objective. In this work, we utilized the embedding vectors from the final hidden layer of ESM2 to characterize each amino acid position in the sequence.

We used the publicly available 150 million parameter ESM2 model for all experiments. ESM2 model was implemented via the HuggingFace Transformers library, and the final protein embedding for each input was obtained as the output of the respective pooling strategy. This approach enables the seamless integration of evolutionary and structural information embedded in ESM2, providing highly informative features for both PHT and PT binding prediction tasks in TCRBinder.

\subsection*{Roformer for TCR Sequence Representation}
The Roformer encoder is employed to capture the long-range dependencies and relative positions within TCR $\alpha$ and $\beta$ chain sequences by leveraging rotary position embeddings (RoPE). This mechanism applies a learnable rotation to each amino acid’s embedding vector according to its position, facilitating the modeling of sequence order and spatial information.
Suppose the input TCR sequence of length L is represented as :
\begin{equation}
\mathcal{Z}_{\mathcal{L}} = \{ h_p \}_{p=1}^{L}
\end{equation}
where $h_p$ is the embedding vector for the p-th amino acid residue.
The self-attention mechanism constructs queries, keys, and values as follows:
\begin{equation}
Q_p = \psi_q(h_p, p), \quad 
K_q = \psi_k(h_q, q), \quad 
V_q = \psi_v(h_q, q)
\end{equation}
where $\psi_q()$ , $\psi_k()$ and $\psi_v()$ are learnable mappings that combine sequence and positional information.
The attention weights between positions $p$ and $q$ are computed as:
\begin{equation}
\alpha_{p,q} = 
\frac{\exp\!\left(\frac{Q_p^\top K_q}{\sqrt{d_k}}\right)}
     {\sum_{t=1}^L \exp\!\left(\frac{Q_p^\top K_t}{\sqrt{d_k}}\right)}
\end{equation}
where $d_k$ is the dimensionality of the key vectors.
To encode relative positions, RoPE rotates the projected embeddings according to position :
\begin{equation}
\psi_q(h_p, p) = (M_q h_p) \cdot \mathrm{rot}(p), 
\quad 
\psi_k(h_q, q) = (M_k h_q) \cdot \mathrm{rot}(q)
\end{equation}
where $M_q$ and $M_k$ are trainable matrices, and $\mathrm{rot}(q)$ denotes a positional rotation operator (sinusoidal transformation).
The attention computation therefore encodes the relative position as:

\begin{equation}
\left\langle \psi_q(h_p, p), \, \psi_k(h_q, q) \right\rangle 
= \phi \big( h_p, \, h_q, \, p - q \big)
\end{equation}
where $\phi()$ is a function that incorporates both the content of the amino acids and their relative distance in the sequence.
This approach allows the model to infer both sequential and structural relationships within TCRs. For TCRBinder, independent Roformer encoders are used for TCR $\alpha$ and $\beta$ chains, and the final representation for each chain is obtained by mean pooling the output of the last hidden layer. Only the final layer parameters are fine-tuned during training to balance adaptation and generalization.

\subsection*{Convolutional fusion and discriminative prediction}
Following the generation of modality-specific embeddings from the four pre-trained encoders, the final discriminative stage of TCRBinder is orchestrated by a dedicated fusion architecture designed to model the complex inter-molecular interactions. Each of the four high-dimensional representations, which derived from the antigen, HLA, TCR $\alpha$ chain, and TCR $\beta$ chain, is independently processed by a (MFCNN) module. This module is engineered to extract salient local motifs and multi-scale features from the token-level embeddings. The core of each MFCNN stream is a sequence of three identical convolutional blocks. A single block's operation on an input tensor Z is defined as:
\begin{equation}
CNN_{\text{Block}}(Z) = \mathrm{MaxPool}\big( \mathrm{GLU}(\mathrm{Conv1D}(Z)))
\end{equation}
where $Conv1D()$ represents a one-dimensional convolution, $GLU()$ is the Gated Linear Unit activation function, and $MaxPool()$ is a max pooling layer. The full multi-scale feature extraction for a given sequence, such as the antigenic peptide $(x^p)$ and  TCR $\alpha$ chain $(x^\alpha)$ , is a composition of these blocks:

\begin{equation}
F_p = (CNN_{(3)}\!( ESM2(x^p) \big)
\end{equation}

\begin{equation}
F'_p = FCN_{(2)}(F_p) + F_p
\end{equation}

\begin{equation}
F_{\alpha} = CNN_{(3)}\!( RoFormer(x^{\alpha}))
\end{equation}

\begin{equation}
F'_{\alpha} = FCN_{(2)}(F_{\alpha}) + F_{\alpha}
\end{equation}

This hierarchical process is executed in parallel for the antigenic peptide $(F'_p)$, HLA $(F'_h)$, TCR $\alpha$ chain $(F'_\alpha)$, and TCR $\beta$ chain $(F'_\beta)$ embeddings. Subsequently, these four refined feature maps are concatenated to form a jonit, holistic feature tensor, $S_F$, which represents the entire quaternary complex:
\begin{equation}
S_F = \mathrm{Concatenate}(F'_p, F'_h, F'_{\alpha}, F'_{\beta})
\end{equation}

To manage the variable lengths inherent in protein sequences, the MFCNN enforces a maximum sequence length. Inputs exceeding this predefined threshold are truncated, while shorter sequences are padded with a designated [PAD] token. This strategy ensures that all inputs are converted into fixed-size tensors, a necessary step for efficient, batched computation in deep learning frameworks.

The high-level joint feature is then passed to the final prediction head, which consists of a MLP layer. Finally, the PHT binding sepcificity $BS$ are obtained.
\begin{equation}
    BS = Sig(MLP(S_F))
\end{equation}
where $Sig()$ is the Sigmoid function.


\subsection*{Key Amino Acid Residue Identification Strategy}

TCRBinder uses full-length paired TCR chains to evaluate binding specificity, but we further focused on the CDR3 loop of the $\beta$ chain, which typically provides the primary contact with the peptide. To support our experiment and verify our findings in 3D space, we screened PHT crystal complexes (N = 112) from the previous research \cite{ref30} . From these PHT complexes, we extracted CDR3, peptides, and HLA sequences. To study the role of the CDR3$\beta$ loop in the binding of PHT, the CDR3 sequence was divided into 5 fragments by using the sliding window method. This method dynamically adjusted the fragment length according to the total sequence length to achieve nearly equal division, and the central fragment was given priority to cover the key contact area. For instance, given a CDR3$\beta$ sequence of length 15, five segments of equal length can be divided. This segmentation was checked against structural contact information to ensure consistency with crystallographic data. Alanine substitutions were then introduced within each segment, and the modified sequences were evaluated using TCRBinder. Binding scores were summarized according to the position of the substituted residues to identify sites most critical for recognition.

\section*{Discussion}
We present TCRBinder, a unified model that integrates a pre-trained language model with a paired chain consistency predictor to predict antigen specificity by learning joint representations of paired TCR $\alpha$ and $\beta$ chains. Its architecture captures binding determinants that may be overlooked by single chain models by leveraging the joint context of both TCR chains during training. Our results demonstrate that this paired chain approach achieves state-of-the-art predictive performance on PHT binding tasks, outperforming models that consider only the TCR $\beta$ chain or employ separate encoders. By pre-training on large-scale TCR repertoire data, TCRBinder incorporates both generalizable sequence patterns and rare binding motifs into its embeddings, significantly improving downstream predictive accuracy compared to traditional clustering or motif-based methods. These findings highlight the architectural advantages of integrating paired chain information and validate the benefits of fully considering both TCR $\alpha$ and TCR $\beta$ chain sequences to binding specificity.


The great improvement of TCRBinder’s performance align with emerging evidence that deep learning and language model embeddings can surpass the accuracy of earlier pTCR prediction tools. Our unified design, which reinforces inter-chain interactions, yields a richer representation of the TCR interface. In benchmark comparisons, this approach demonstrates enhanced generalization, this may due to the model captures paired chain concordance and subtle inter-chain dependencies that are lost when processing the chains independently. Furthermore, the inclusion of both chains aligns with the biological reality of the TCR $\alpha$ and $\beta$ chains acting as a single recognition unit and is consistent with previous findings where incorporating TCR $\alpha$ chain data significantly boosted performance. Moreover, by leveraging a pre-trained language model foundation, TCRBinder is capable of identifying biochemically relevant sequence features, such as conserved motifs or physicochemical preferences, similar to recent TCR-specific language models that capture amino acid properties and positional biases. To illustrate TCRBinder's practical potential, we conducted in-depth analyses, including identifying TCR clusters specific to certain antigens and estimating SARS-CoV-2 spike and non-spike specific T-cell responses. These strengths underscore the methodological advances of TCRBinder over approaches based purely on sequence similarity or shallow machine learning.


Despite these advances, limitations in generalizability and interpretability remain. Like other deep learning predictors, TCRBinder’s performance, while superior to other methods, still needs improvement for robust clinical application, particularly when encountering epitopes outside its training distribution. When tested on entirely novel peptides or TCRs whose sequences diverge substantially from the training data, prediction accuracy may degrade to near-random levels. This reflects the immense diversity of the pTCR space and indicates that the model has not yet inferred truly universal binding rules. Furthermore, our training data, sourced from public databases, contains inherent biases, including the over-representation of certain viral epitopes and a scarcity of paired TCR $\alpha$ chain sequences, both of which may inflate performance in these specific contexts while limiting it in others. This data imbalance, which is coupled with the model's reliance on sequence features, implies that certain factors, such as specific HLA allele effects or TCR conformational dynamics, are not explicitly encoded, which in turn limits predictive power in edge cases.


Future work will focus on addressing these limitations to further optimize TCRBinder. We plan to incorporate structure-based features, which could better capture the physical principles of binding by combining sequence embeddings with structural context, thereby enabling the recognition of truly novel antigen patterns. Expanding the training dataset is equally critical. Encompassing a greater diversity of epitopes, such as under-represented pathogens and tumor neoantigens, and ensuring all entries have paired chain data will help mitigate bias and broaden the model's adaptability to different specificities. Finally, through continuous integration of new data and domain knowledge, we aim to move closer toward the goal of mapping the complete TCR–antigen recognition landscape. This will significantly accelerate vaccine design, immunotherapy optimization, and our fundamental understanding of adaptive immunity.

\section*{Supporting information}

% Include only the SI item label in the paragraph heading. Use the \nameref{label} command to cite SI items in the text.
\paragraph*{S1 Text.} \label{S1_Text}
\textbf{Supplementary materials.}




\section*{Author contributions}
\setlength{\parindent}{0pt}
\textbf{Conceptualization}: Weihe Dong Xiaokun Li.

\textbf{Data curation}: Weihe Dong, Long Xu.

\textbf{Formal analysis}: Qiang Yang, Suyu Dong.

\textbf{Funding acquisition}: Xiaokun Li, Kuanquan Wang, Gongning Luo.

\textbf{Investigation}: Weihe Dong, Qiang Yang.

\textbf{Methodology}: Weihe Dong.

\textbf{Project administration}: Weihe Dong, Xin Gao, Guohua Wang.

\textbf{Supervision}: Xiaokun Li, Xin Gao, Guohua Wang.

\textbf{Validation}: Qiang Yang.

\textbf{Writing -- original draft}: Weihe Dong.

\textbf{Writing -- review \& editing}: Xianyu Zhang, Tiansong Yang.


\section*{Funding}
This work was supported by the Natural Science Foundation of Heilongjiang Province of China (ZD2024F001 to GW), National Natural Science Foundation of China (62225109 to GW, 62450122 to GW, 62372135 to GL, 62202092 to SD), and the King Abdullah University of Science and Technology (KAUST) Office of Research Administration (ORA) under Award No REI/1/5234-01-01, REI/1/5414-01-01, REI/1/5289-01-01, REI/1/5404-01-01, REI/1/5992-01-01, URF/1/4663-01-01, Center of Excellence for Smart Health (KCSH) under award number 5932, and Center of Excellence on Generative AI under award number 5940 to XG. The funders
did not play any role in the study design, data collection and analysis, decision to publish, or preparation of the manuscript.


\section*{Data Availability}
TCRBinder is available on GitHub \href{https://github.com/dongweihe/TCRBinder}{(https://github.com/dongweihe/TCRBinder)}. The raw interaction data were compiled from publicly accessible resources, including VDJdb \href{https://vdjdb.cdr3.net/} {(https://vdjdb.cdr3.net/)}, IEDB \href{https://www.iedb.org/}{(https://www.iedb.org/)}, McPAS-TCR \href{http://friedmanlab.weizmann.ac.il/McPAS-TCR/}{(http://friedmanlab.weizmann.ac.il/McPAS-TCR/)}, OTS \href{https://opig.stats.ox.ac.uk/webapps/ots}{(https://opig.stats.ox.ac.uk/webapps/ots)}, and the 10x Genomics datasets \href{https://www.10xgenomics.com/datasets}{(https://www.10xgenomics.com/datasets)}.


\section*{Competing interests}
The authors have declared that no competing interests exist.

\nolinenumbers

% Either type in your references using
% \begin{thebibliography}{}
% \bibitem{}
% Text
% \end{thebibliography}
%
% or
%
% Compile your BiBTeX database using our plos2015.bst
% style file and paste the contents of your .bbl file
% here. See http://journals.plos.org/plosone/s/latex for 
% step-by-step instructions.
% 
\begin{thebibliography}{10}

\bibitem{ref1}
Ribas A, et al.
\newblock {Cancer immunotherapy using checkpoint blockade}.
\newblock Science. 2018;359:1350--1355.

\bibitem{ref2}
Waldman AD, et al.
\newblock {A guide to cancer immunotherapy: from T cell basic science to clinical practice}.
\newblock Nat Rev Immunol. 2020;20:651--668.

\bibitem{ref3}
Sharma P, et al.
\newblock {Dissecting the mechanisms of immune checkpoint therapy}.
\newblock Nat Rev Immunol. 2020;20:75--76.

\bibitem{ref4}
Chen DS, et al.
\newblock {Elements of cancer immunity and the cancer-immune set point}.
\newblock Nature. 2017;541:321--330.

\bibitem{ref5}
Hegde PS, et al.
\newblock {Top 10 challenges in cancer immunotherapy}.
\newblock Immunity. 2020;52:17--35.

\bibitem{ref6}
Leko V, et al.
\newblock {Identifying and targeting human tumor antigens for T cell-based immunotherapy of solid tumors}.
\newblock Cancer Cell. 2020;38:454--472.

\bibitem{ref7}
Ott PA, et al.
\newblock {An immunogenic personal neoantigen vaccine for patients with melanoma}.
\newblock Nature. 2017;547:217--221.

\bibitem{ref8}
Sahin U, et al.
\newblock {Personalized RNA mutanome vaccines mobilize poly-specific therapeutic immunity against cancer}.
\newblock Nature. 2017;547:222--226.

\bibitem{ref9}
Lowery FJ, et al.
\newblock {Molecular signatures of antitumor neoantigen-reactive T cells from metastatic human tumors}.
\newblock Science. 2022;375:877--884.

\bibitem{ref10}
Chen DS, et al.
\newblock {Oncology meets immunology: the cancer-immunity cycle}.
\newblock Immunity. 2013;39:1--10.

\bibitem{ref11}
Baulu E, et al.
\newblock {TCR-engineered T cell therapy in solid tumors: State of the art and perspectives}.
\newblock Sci Adv. 2023;9:eadf3700.

\bibitem{ref12}
Caushi JX, et al.
\newblock {Transcriptional programs of neoantigen-specific TIL in anti-PD-1-treated lung cancers}.
\newblock Nature. 2021;596:126--132.

\bibitem{ref13}
Oliveira G, et al.
\newblock {Phenotype, specificity and avidity of antitumour CD8+ T cells in melanoma}.
\newblock Nature. 2021;596:119--125.

\bibitem{ref14}
Foy SP, et al.
\newblock {Non-viral precision T cell receptor replacement for personalized cell therapy}.
\newblock Nature. 2023;615:687--696.

\bibitem{ref15}
Robins H, et al.
\newblock {Immunosequencing: applications of immune repertoire deep sequencing}.
\newblock Curr Opin Immunol. 2013;25:646--652.

\bibitem{ref16}
Carter JA, et al.
\newblock {Single T cell sequencing demonstrates the functional role of $\alpha\beta$ TCR pairing in cell lineage and antigen specificity}.
\newblock Front Immunol. 2019;10:3226.

\bibitem{ref17}
Lagattuta KA, et al.
\newblock {Repertoire analyses reveal T cell antigen receptor sequence features that influence T cell fate}.
\newblock Nat Immunol. 2022;23:446--457.

\bibitem{ref18}
Chen L, et al.
\newblock {Human TCR repertoire in cancer}.
\newblock Cancer Med. 2024;13:e70164.

\bibitem{ref19}
Rubelt F, et al.
\newblock {Adaptive Immune Receptor Repertoire Community recommendations for sharing immune-repertoire sequencing data}.
\newblock Nat Immunol. 2017;18:1274.

\bibitem{ref20}
Musvosvi M, et al.
\newblock {T cell receptor repertoires associated with control and disease progression following Mycobacterium tuberculosis infection}.
\newblock Nat Med. 2023;29:258--269.

\bibitem{ref21}
Lu T, et al.
\newblock {Deep learning-based prediction of the T cell receptor–antigen binding specificity}.
\newblock Nat Mach Intell. 2021;3:864--875.

\bibitem{ref22}
Yang Q, et al.
\newblock {HLAIImaster: a deep learning method with adaptive domain knowledge predicts HLA II neoepitope immunogenic responses}.
\newblock Brief Bioinform. 2024;25:bbae302.

\bibitem{ref23}
Xu L, et al.
\newblock {Meta learning for mutant HLA class I epitope immunogenicity prediction to accelerate cancer clinical immunotherapy}.
\newblock Brief Bioinform. 2025;26:bbae625.

\bibitem{ref24}
Reynisson B, et al.
\newblock {NetMHCpan-4.1 and NetMHCIIpan-4.0: improved predictions of MHC antigen presentation by concurrent motif deconvolution and integration of MS MHC eluted ligand data}.
\newblock Nucleic Acids Res. 2020;48:W449--W454.

\bibitem{ref25}
O'Donnell TJ, et al.
\newblock {MHCflurry 2.0: Improved Pan-Allele Prediction of MHC Class I-Presented Peptides by Incorporating Antigen Processing}.
\newblock Cell Syst. 2020;11:42--48.e7.

\bibitem{ref26}
Moris P, et al.
\newblock {Current challenges for unseen-epitope TCR interaction prediction and a new perspective derived from image classification}.
\newblock Brief Bioinform. 2021;22:bbaa318.

\bibitem{ref27}
Peng X, et al.
\newblock {Characterizing the interaction conformation between T-cell receptors and epitopes with deep learning}.
\newblock Nat Mach Intell. 2023;5:395--407.

\bibitem{ref28}
Zhang W, et al.
\newblock {A framework for highly multiplexed dextramer mapping and prediction of T cell receptor sequences to antigen specificity}.
\newblock Sci Adv. 2021;7:eabf5835.

\bibitem{ref29}
Lu T, et al.
\newblock {Deep learning-based prediction of the T cell receptor–antigen binding specificity}.
\newblock Nat Mach Intell. 2021;3:864--875.

\bibitem{ref30}
Long X, et al.
\newblock {THLANet: A deep learning framework for predicting TCR-pHLA binding in immunotherapy applications}.
\newblock PLoS Comput Biol. 2025;21:e1013050.

\bibitem{ref31}
Goncharov M, et al.
\newblock {VDJdb in the pandemic era: a compendium of T cell receptors specific for SARS-CoV-2}.
\newblock Nat Methods. 2022;19:1017--1019.

\bibitem{ref32}
Vita R, et al.
\newblock {The Immune Epitope Database (IEDB): 2018 update}.
\newblock Nucleic Acids Res. 2019;47:D339--D343.

\bibitem{ref33}
Dolton G, et al.
\newblock {Targeting of multiple tumor-associated antigens by individual T cell receptors during successful cancer immunotherapy}.
\newblock Cell. 2023;186:3333--3349.e27.

\bibitem{ref34}
Dong R, et al.
\newblock {Structural analysis of cancer-relevant TCR-CD3 and peptide-MHC complexes}.
\newblock Nat Commun. 2023;14:2496.

\bibitem{ref35}
Zhang M, et al.
\newblock {Identification and affinity enhancement of T-cell receptor targeting a KRASG12V cancer neoantigen}.
\newblock Commun Biol. 2024;7:512.

\bibitem{ref36}
Bhat S, et al.
\newblock {De novo design of peptide binders to conformationally diverse targets with contrastive language modeling}.
\newblock Sci Adv. 2025;11:eadr8638.

\bibitem{ref37}
Kwee BPY, et al.
\newblock {STAPLER: efficient learning of TCR-peptide specificity prediction from full-length TCR-peptide data}.
\newblock bioRxiv. 2023;04.25.538237.

\bibitem{ref38}
Jensen MF, et al.
\newblock {Enhancing TCR specificity predictions by combined pan- and peptide-specific training, loss-scaling, and sequence similarity integration}.
\newblock eLife. 2024;12:RP93934.

\bibitem{ref39}
Croce G, et al.
\newblock {Deep learning predictions of TCR-epitope interactions reveal epitope-specific chains in dual alpha T cells}.
\newblock Nat Commun. 2024;15:3211.

\bibitem{ref40}
Montemurro A, et al.
\newblock {NetTCR-2.0 enables accurate prediction of TCR-peptide binding by using paired TCR $\alpha$ and $\beta$ sequence data}.
\newblock Commun Biol. 2021;4:1060.

\bibitem{ref41}
Minervina AA, et al.
\newblock {SARS-CoV-2 antigen exposure history shapes phenotypes and specificity of memory CD8+ T cells}.
\newblock Nat Immunol. 2022;23:781--790.

\bibitem{ref42}
10x Genomics, et al.
\newblock {A new way of exploring immunity – linking highly multiplexed antigen recognition to immune repertoire and phenotype}.
\newblock 10x Genomics. 2022.

\bibitem{ref43}
Raybould MIJ, et al.
\newblock {The Observed T Cell Receptor Space database enables paired-chain repertoire mining, coherence analysis, and language modeling}.
\newblock Cell Rep. 2024;43:114704.

\bibitem{ref44}
Tickotsky N, et al.
\newblock {McPAS-TCR: a manually curated catalogue of pathology-associated T cell receptor sequences}.
\newblock Bioinformatics. 2017;33:2924--2929.

\end{thebibliography}


\end{document}

